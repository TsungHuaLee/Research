{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as albu\n",
    "import os, glob, sys, shutil\n",
    "import cv2, itertools, random, pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from collections import Counter\n",
    "from numpy.random import choice\n",
    "from utils.meter import AverageValueMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_process\n",
    "import utils\n",
    "IMAGE_FOLDER = \"/data/tcga/512dense/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data/cohort_high_low.pkl\", \"rb\") as fp:\n",
    "    cohort_count_dict = pickle.load(fp)\n",
    "print(\"# cohort: {}\".format(len(cohort_count_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = list(cohort_count_dict.keys())\n",
    "patient_cls = list(cohort_count_dict.values())\n",
    "lookup = dict(zip(patient_ids, patient_cls))\n",
    "Counter(patient_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patient, valid_patient = train_test_split(patient_ids, test_size = 0.2, random_state = 42)\n",
    "train_cls = [lookup[i] for i in train_patient]\n",
    "valid_cls = [lookup[i] for i in valid_patient]\n",
    "print(\"# train patient:{}\\n# valid patient:{}\".format(Counter(train_cls), Counter(valid_cls)))\n",
    "del train_cls\n",
    "del valid_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, valid_images, train_lookup, valid_lookup, train_npys = utils.data.load_data(train_patient=train_patient, valid_patient=valid_patient, \\\n",
    "                                                        patient_label_dict = lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def get_preprocessing():\n",
    "    _transform = [\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)\n",
    "\n",
    "def get_training_augmentation():\n",
    "    test_transform = [\n",
    "        albu.Resize(224, 224),\n",
    "        albu.Flip(),\n",
    "        albu.ShiftScaleRotate(border_mode=0, value=0),\n",
    "        albu.HueSaturationValue(hue_shift_limit=40, p=0.8),\n",
    "        albu.RandomBrightnessContrast(p=0.8),\n",
    "        albu.IAAAdditiveGaussianNoise(),\n",
    "        albu.GaussianBlur(),\n",
    "        albu.Normalize()\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    test_transform = [\n",
    "        albu.Resize(224, 224),\n",
    "        albu.Normalize()\n",
    "        #albu.ToGray(p = 1.)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_labeled_unlabeled(train_filtered_label = None):\n",
    "    unlabeled_patch = []\n",
    "    labeled_patch = []\n",
    "    num_p = 0\n",
    "    for key, value in train_filtered_label.items():\n",
    "        if value == -1:\n",
    "            unlabeled_patch.append(\"{}\".format(key))\n",
    "        else:\n",
    "            labeled_patch.append(\"{}\".format(key))\n",
    "            num_p += value\n",
    "    \n",
    "    return labeled_patch, unlabeled_patch, num_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, patches, patch_cls_lookup, augmentation = None, preprocessing = None):\n",
    "        self.patches = patches\n",
    "        self.patch_cls_lookup = patch_cls_lookup\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        image_name = self.patches[i]\n",
    "        svs_name = image_name.split(\"_\")[0]\n",
    "        full_path = os.path.join(IMAGE_FOLDER, svs_name, image_name+\".jpg\")\n",
    "        image = data_process.wsi_utils.vips_get_image(full_path)\n",
    "        \n",
    "        cls = self.patch_cls_lookup[image_name]\n",
    "        \n",
    "        if(self.augmentation):\n",
    "            sampled = self.augmentation(image = image)\n",
    "            s_input = sampled['image']\n",
    "            \n",
    "        if(self.preprocessing):\n",
    "            sampled = self.preprocessing(image = s_input)\n",
    "            s_input = sampled['image']\n",
    "            \n",
    "        return s_input, cls, image_name\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, patches, augmentation = None, preprocessing = None):\n",
    "        self.patches = patches\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        image_name = self.patches[i]\n",
    "        svs_name = image_name.split(\"_\")[0]\n",
    "        full_path = os.path.join(IMAGE_FOLDER, svs_name, image_name+\".jpg\")\n",
    "        image = data_process.wsi_utils.vips_get_image(full_path)\n",
    "        \n",
    "        if(self.augmentation):\n",
    "            sampled = self.augmentation(image = image)\n",
    "            _input1 = sampled['image']\n",
    "            sampled = self.augmentation(image = image)\n",
    "            _input2 = sampled['image']\n",
    "            \n",
    "        if(self.preprocessing):\n",
    "            sampled = self.preprocessing(image = _input1, mask = _input2)\n",
    "            _input1 = sampled['image']\n",
    "            _input2 = sampled['mask']\n",
    "            \n",
    "        return _input1, _input2\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Epoch:\n",
    "    def __init__(self, model, loss, metrics, stage_name, device='cpu', verbose=True):\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.stage_name = stage_name\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "\n",
    "        self._to_device()\n",
    "\n",
    "    def _to_device(self):\n",
    "        self.model.to(self.device)\n",
    "        self.loss.to(self.device)\n",
    "        for metric in self.metrics:\n",
    "            metric.to(self.device)\n",
    "\n",
    "    def _format_logs(self, logs):\n",
    "        str_logs = ['{} - {:.4}'.format(k, v) for k, v in logs.items()]\n",
    "        s = ', '.join(str_logs)\n",
    "        return s\n",
    "\n",
    "    def batch_update(self, x, y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def on_epoch_start(self):\n",
    "        pass\n",
    "    \n",
    "    def run(self, dataloader, epoch):\n",
    "        self.on_epoch_start()\n",
    "\n",
    "        logs = {}\n",
    "        loss_meter = AverageValueMeter()\n",
    "        metrics_meters = {metric.__name__: AverageValueMeter() for metric in self.metrics}\n",
    "        patient_pred = {}\n",
    "        with tqdm(dataloader, desc=self.stage_name, file=sys.stdout, disable=not (self.verbose)) as iterator:\n",
    "            for x, y, patch_name in iterator:\n",
    "                patch_name = list(patch_name)\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                loss_value, preds = self.batch_update(x, y)\n",
    "                assert torch.isnan(loss_value).any().detach().cpu().numpy() != True, 'loss value 有問題 {}'.format(loss_value)\n",
    "                \n",
    "                # update loss logs\n",
    "                loss_value = loss_value.cpu().detach().numpy()\n",
    "                loss_meter.add(loss_value)\n",
    "                loss_logs = {self.loss.__name__: loss_meter.mean}\n",
    "                logs.update(loss_logs)\n",
    "                \n",
    "                # update metrics logs\n",
    "                for metric_fn in self.metrics:\n",
    "                    metric_value = metric_fn(preds, y).cpu().detach().numpy()\n",
    "                    metrics_meters[metric_fn.__name__].add(metric_value)\n",
    "                metrics_logs = {k: v.mean for k, v in metrics_meters.items()}\n",
    "                logs.update(metrics_logs)                \n",
    "                \n",
    "                if self.verbose:\n",
    "                    s = self._format_logs(logs)\n",
    "                    iterator.set_postfix_str(s)\n",
    "                    \n",
    "                preds = torch.softmax(preds, dim=1)\n",
    "                preds = preds[:, 1].detach().cpu().numpy()\n",
    "                for p, p_n in zip(preds, patch_name):\n",
    "                    p_id = p_n[:12]\n",
    "                    if(p_id not in patient_pred):\n",
    "                        patient_pred[p_id] = [0, 0]\n",
    "                    if(p > 0.5):\n",
    "                        patient_pred[p_id][1] += 1\n",
    "                    else:\n",
    "                        patient_pred[p_id][0] += 1\n",
    "                    \n",
    "        y_pred = []\n",
    "        y_gt = []\n",
    "        # voting\n",
    "        for key, values in patient_pred.items():\n",
    "            y_gt.append(lookup[key])\n",
    "            d = values[0] + values[1]\n",
    "            y_pred.append(values[1] / d)\n",
    "            \n",
    "        auc = sklearn.metrics.roc_auc_score(y_gt, y_pred)\n",
    "        precision, recall, _thresholds = sklearn.metrics.precision_recall_curve(y_gt, y_pred)\n",
    "        aupr = sklearn.metrics.auc(recall, precision)\n",
    "        \n",
    "        print('patient AUROC : ', auc)\n",
    "        print('patient AUPR : ', aupr)\n",
    "        logs.update({'auc' : auc})\n",
    "        logs.update({'aupr' : aupr})\n",
    "        writer.add_scalar(\"{}/{}\".format(self.loss.__name__, self.stage_name), loss_meter.mean, epoch)\n",
    "        writer.add_scalar(\"{}/{}\".format(\"Patient AUC\", self.stage_name), auc, epoch)\n",
    "        writer.add_scalar(\"{}/{}\".format(\"Patient AUPR\", self.stage_name), aupr, epoch)\n",
    "        writer.add_scalar(\"{}/{}\".format(self.metrics[0].__name__, self.stage_name), metrics_meters[self.metrics[0].__name__].mean, epoch)\n",
    "        \n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialTrainEpoch(Epoch):\n",
    "\n",
    "    def __init__(self, model, loss, metrics, optimizer, device='cpu', verbose=True):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            stage_name='Initial train',\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def on_epoch_start(self):\n",
    "        self.model.train()\n",
    "\n",
    "    def batch_update(self, x, y):\n",
    "        self.optimizer.zero_grad()\n",
    "        prediction = self.model.forward(x)\n",
    "        loss = self.loss(prediction, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss, prediction\n",
    "    \n",
    "class ValidEpoch(Epoch):\n",
    "\n",
    "    def __init__(self, model, loss, metrics, device='cpu', verbose=True):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            stage_name='valid',\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    def on_epoch_start(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def batch_update(self, x, y):\n",
    "        with torch.no_grad():\n",
    "            prediction = self.model.forward(x)\n",
    "            loss = self.loss(prediction, y)\n",
    "        return loss, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterEpoch(Epoch):\n",
    "    \"\"\"\n",
    "    SELF\n",
    "    \"\"\"\n",
    "    def __init__(self, model, init_lookup, loss, metrics, ensemble_momentum=0.5, device='cpu', verbose=True):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            stage_name='Filtered',\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.init_lookup = init_lookup\n",
    "        self.ensemble_momentum = ensemble_momentum\n",
    "    def _to_device(self):\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def on_epoch_start(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def run(self, dataloader, epoch):\n",
    "        \"\"\"\n",
    "        dataloader: unshuffle train dataloader\n",
    "        \"\"\"\n",
    "        self.on_epoch_start()\n",
    "        filtered_label = self.init_lookup.copy()\n",
    "        with tqdm(dataloader, desc=self.stage_name, file=sys.stdout, disable=not (self.verbose)) as iterator: \n",
    "            for x, y, patch_name in iterator:\n",
    "                patch_name = list(patch_name)\n",
    "                x = x.to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    predictions = self.model(x)\n",
    "\n",
    "                # predictions:  A `Tensor` of shape [batch_size, class_number]\n",
    "                predictions = torch.softmax(predictions, dim = 1)\n",
    "                predictions = predictions.detach().cpu().numpy()\n",
    "\n",
    "                for pred, p_id in zip(predictions, patch_name):\n",
    "                    predict_running_average[p_id] = self.ensemble_momentum * predict_running_average[p_id] +\\\n",
    "                                                    (1 - self.ensemble_momentum)*pred\n",
    "                    # determine which class is agreement\n",
    "                    agreement = np.argmax(predict_running_average[p_id])\n",
    "                    # agreement class != initial class\n",
    "                    if agreement != self.init_lookup[p_id]:\n",
    "                        filtered_label[p_id] = -1\n",
    "        return filtered_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewFilterEpoch(Epoch):\n",
    "    def __init__(self, model, init_lookup, loss, metrics, ensemble_momentum=0.5, device='cpu', verbose=True):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            stage_name='Filtered',\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.init_lookup = init_lookup\n",
    "        self.ensemble_momentum = ensemble_momentum\n",
    "        \n",
    "    def _to_device(self):\n",
    "        self.model.to(self.device)\n",
    "        self.loss.to(self.device)\n",
    "        \n",
    "    def on_epoch_start(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def SELF_filter_noisy(self, patch_names, patch_preds, new_filtered_label):\n",
    "        for patch_pred, p_id in zip(patch_preds, patch_names):\n",
    "            p_id = p_id\n",
    "            predict_running_average[p_id] = self.ensemble_momentum * predict_running_average[p_id] +\\\n",
    "                                            (1 - self.ensemble_momentum)*patch_pred\n",
    "            # determine which class is agreement\n",
    "            agreement = np.argmax(predict_running_average[p_id])\n",
    "            # agreement class != initial class\n",
    "            if agreement != self.init_lookup[p_id]:\n",
    "                new_filtered_label[p_id] = -1\n",
    "                \n",
    "    def GMM_filter_noisy(self, patch_names, patch_losses, patch_preds, new_filtered_label):\n",
    "        \"\"\" normalize loss\"\"\"\n",
    "        patch_losses = (patch_losses - patch_losses.min())/(patch_losses.max() - patch_losses.min() + 1e-10)\n",
    "        \"\"\" fit a two-component GMM to the loss \"\"\"\n",
    "        gmm = GaussianMixture(n_components=2,max_iter=10,tol=1e-2,reg_covar=5e-4)\n",
    "        input_losses = patch_losses.reshape(-1, 1)\n",
    "        gmm.fit(input_losses)\n",
    "        \"\"\" minimize loss\"\"\"\n",
    "        gmm_preds = gmm.predict_proba(input_losses) \n",
    "        gmm_preds = gmm_preds[:,gmm.means_.argmin()]\n",
    "        gmm_preds = np.where(gmm_preds > 0.5, 1, 0)\n",
    "        for gmm_pred, patch_pred, p_id in zip(gmm_preds, patch_preds, patch_names):\n",
    "            p_id = p_id\n",
    "            new_filtered_label[p_id] = gmm_pred\n",
    "            \"\"\" 繼續ensemble \"\"\"\n",
    "            predict_running_average[p_id] = self.ensemble_momentum * predict_running_average[p_id] +\\\n",
    "                                            (1 - self.ensemble_momentum)*patch_pred\n",
    "        \n",
    "    def run(self, all_npys, epoch):\n",
    "        self.on_epoch_start()\n",
    "        new_filtered_label = self.init_lookup.copy()\n",
    "        \n",
    "        with tqdm(all_npys, desc=self.stage_name, file=sys.stdout, disable=not (self.verbose)) as iterator:\n",
    "            for npy in iterator:\n",
    "                x_y_pairs = np.load(npy)\n",
    "                svs_name = npy.split(\"/\")[-1][:-4]\n",
    "                patient_name = npy.split(\"/\")[-1][:12]\n",
    "                \n",
    "                image_names = [\"{}_{}_{}\".format(svs_name, x, y) for x, y in x_y_pairs]\n",
    "                test_dataset = LabeledDataset(\n",
    "                    image_names,\n",
    "                    self.init_lookup,\n",
    "                    augmentation = get_validation_augmentation(),\n",
    "                    preprocessing = get_preprocessing(),\n",
    "                )\n",
    "                test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=10, pin_memory=True)\n",
    "                \"\"\" calculate preds and losses \"\"\"\n",
    "                all_preds = None\n",
    "                all_losses = None\n",
    "                for images, labels, patch_name in test_loader:\n",
    "                    patch_name = list(patch_name)\n",
    "                    images, labels = images.to(self.device), labels.cuda(self.device)\n",
    "                    with torch.no_grad():\n",
    "                        preds = self.model(images)\n",
    "                    \"\"\" Loss 'reduction' should be \"none\" \"\"\"\n",
    "                    loss_values = self.loss(preds, labels).detach().cpu().numpy() \n",
    "                    preds = torch.softmax(preds, dim = 1).detach().cpu().numpy()\n",
    "                    assert len(loss_values) == len(preds), \"loss values 數量不同\"\n",
    "                    if all_losses is None:\n",
    "                        all_losses = loss_values\n",
    "                        all_preds = preds\n",
    "                    else:\n",
    "                        all_losses = np.concatenate((all_losses, loss_values))\n",
    "                        all_preds = np.concatenate((all_preds, preds))        \n",
    "                \"\"\"\n",
    "                    1) if GT is positive, and majority voting > 0.3 -> SELF\n",
    "                    2) if GT is positive, and majority voting < 0.3 -> GMM\n",
    "                    3) if GT is negatuve -> SELF\n",
    "                \"\"\"\n",
    "                assert all_preds is not None, \"all_preds get error {}\".format(all_preds.dtype)\n",
    "                majority_voting = np.count_nonzero(np.where(all_preds > 0.5))/len(all_preds)\n",
    "                \n",
    "                if lookup[patient_name] == 0:\n",
    "                    self.SELF_filter_noisy(image_names, all_preds, new_filtered_label)\n",
    "                elif majority_voting > 0.3:\n",
    "                    self.SELF_filter_noisy(image_names, all_preds, new_filtered_label)\n",
    "                else:    \n",
    "                    self.GMM_filter_noisy(image_names, all_losses, all_preds, new_filtered_label)\n",
    "        print(\"filter: {}\".format(Counter(list(new_filtered_label.values()))))\n",
    "        return new_filtered_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainEpoch(utils.train.Epoch):\n",
    "\n",
    "    def __init__(self, model, loss_xent_fn, loss_mse_fn, optimizer, loss=None, metrics=None, \\\n",
    "                 T=0.5, lambda_u=1, alpha=0.75, device='cpu', verbose=True):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            stage_name='Iteration train',\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.loss_xent_fn = loss_xent_fn\n",
    "        self.loss_mse_fn = loss_mse_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.T = T\n",
    "        self.lambda_u = lambda_u\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _to_device(self):\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def on_epoch_start(self):\n",
    "        self.model.train()\n",
    "\n",
    "    def batch_update(self, XandU, p, q):\n",
    "#         with torch.autograd.detect_anomaly():\n",
    "        self.optimizer.zero_grad()\n",
    "        prediction = self.model(XandU)\n",
    "        loss_xent_value = self.loss_xent_fn(prediction[:len(p)], p)\n",
    "        loss_mse_value = self.loss_mse_fn(prediction[len(p):], q)\n",
    "        loss = loss_xent_value + self.lambda_u * loss_mse_value\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss_xent_value, loss_mse_value, prediction\n",
    "    \n",
    "    def run(self, labeled_dataloader, unlabeled_dataloader, epoch):\n",
    "        logs = {}\n",
    "        loss_xent_meters = AverageValueMeter()\n",
    "        loss_mse_meters = AverageValueMeter()\n",
    "        \n",
    "#         with tqdm(unlabeled_dataloader, desc=self.stage_name, file=sys.stdout, disable=not (self.verbose)) as unlabeled_iter:\n",
    "#             labeled_iter = iter(labeled_dataloader)\n",
    "#             for (inputs_u, inputs_u2) in unlabeled_iter:            \n",
    "#                 try:\n",
    "#                     inputs_x, targets_x, names= labeled_iter.next()\n",
    "#                 except:\n",
    "#                     labeled_iter = iter(labeled_dataloader)\n",
    "#                     inputs_x, targets_x, names = labeled_iter.next()        \n",
    "        with tqdm(labeled_dataloader, desc=self.stage_name, file=sys.stdout, disable=not (self.verbose)) as labeled_iter:\n",
    "            unlabeled_iter = iter(unlabeled_dataloader)\n",
    "            for (inputs_x, targets_x, names) in labeled_iter:\n",
    "                try:\n",
    "                    inputs_u, inputs_u2 = unlabeled_iter.next()\n",
    "                except:\n",
    "                    unlabeled_iter = iter(unlabeled_dataloader)\n",
    "                    inputs_u, inputs_u2 = unlabeled_iter.next()\n",
    "                    \n",
    "                inputs_x, targets_x = inputs_x.to(self.device), targets_x.to(self.device)\n",
    "                inputs_u, inputs_u2 = inputs_u.to(self.device), inputs_u2.to(self.device)\n",
    "                \"\"\"\n",
    "                MixMatch:\n",
    "                    inputs_x: shape `(N//2, channel, H, W)`\n",
    "                    targets_x: shape `(N//2, )`\n",
    "                    outputs_u: shape `(N//2, channel, H, W)`\n",
    "                    outputs_u2: shape `(N//2, channel, H, W)`\n",
    "                    targets_u: shape `(N//2, C)`\n",
    "                \"\"\"\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    outputs_u = self.model(inputs_u)\n",
    "                    outputs_u2 = self.model(inputs_u2)\n",
    "                    # Compute average predictions\n",
    "                    targets_u = (torch.softmax(outputs_u, dim=1) + torch.softmax(outputs_u2, dim=1)) / 2\n",
    "                    # Apply temperature sharpen\n",
    "                    targets_u = targets_u**(1/self.T)\n",
    "                    targets_u = targets_u / targets_u.sum(dim=1, keepdim=True)\n",
    "                    targets_u = targets_u.detach()\n",
    "                \n",
    "                \"\"\"\n",
    "                convert targets_x to one hot (N//2, C)\n",
    "                indices = (N//2 + N//2 + N//2), 打亂順序給mixup用\n",
    "                Wx: shape `(3*N//2, channel, H, W)`, 透過indices shuffle過的labeled inputs + unlabeled inputs\n",
    "                Wy: shape `(3*N//2, C)`, 透過indices shuffle過的labeled + unlabeled\n",
    "                X: shape `(N//2, channel, H, W)`\n",
    "                p: shape `(N//2, C)`\n",
    "                U: shape `(N//2, channel, H, W)`\n",
    "                q: shape `(N//2, C)`\n",
    "                \"\"\"\n",
    "                targets_x = get_tensor_onehot(targets_x)\n",
    "                indices = np.random.choice(np.arange(len(targets_x)+len(targets_u)+len(targets_u)),\\\n",
    "                                           len(targets_x)+len(targets_u)+len(targets_u))\n",
    "                Wx = torch.cat([inputs_x, inputs_u, inputs_u2], dim=0)[indices]\n",
    "                Wy = torch.cat([targets_x, targets_u, targets_u], dim=0)[indices]\n",
    "\n",
    "                X, p = mixup(inputs_x, Wx[:len(inputs_x)], targets_x, Wy[:len(inputs_x)], self.alpha)\n",
    "                U, q = mixup(inputs_u, Wx[len(inputs_x):len(inputs_u)+len(inputs_x)], \\\n",
    "                             targets_u, Wy[len(inputs_x):len(targets_u)+len(inputs_x)], self.alpha)\n",
    "                \"\"\"\n",
    "                Model Training:\n",
    "                    outputs_u: shape `(N, channel, H, W)`\n",
    "                    p: labeled data,  shape `(N//2, C)`\n",
    "                    q: unlabeled data,  shape `(N//2, C)`\n",
    "                \"\"\"\n",
    "                self.model.train()\n",
    "                # update\n",
    "                loss_xent_value, loss_mse_value, preds = self.batch_update(torch.cat([X,U], dim=0), p, q)\n",
    "                \n",
    "                if torch.isnan(loss_xent_value).any().detach().cpu().numpy() == True:\n",
    "                    print(\"xent ERROR\")\n",
    "                    break\n",
    "                if torch.isnan(loss_mse_value).any().detach().cpu().numpy() == True:\n",
    "                    print(\"mse ERROR\")\n",
    "                    break\n",
    "                \n",
    "                # update loss logs\n",
    "                loss_xent_value = loss_xent_value.cpu().detach().numpy()\n",
    "                loss_xent_meters.add(loss_xent_value)\n",
    "                loss_mse_value = loss_mse_value.cpu().detach().numpy()\n",
    "                loss_mse_meters.add(loss_mse_value)\n",
    "                \n",
    "                logs.update({self.loss_xent_fn.__name__: loss_xent_meters.mean})\n",
    "                logs.update({self.loss_mse_fn.__name__: loss_mse_meters.mean})\n",
    "                \n",
    "                if self.verbose:\n",
    "                    s = self._format_logs(logs)\n",
    "                    labeled_iter.set_postfix_str(s)\n",
    "            writer.add_scalar(\"{}/{}\".format(self.loss_xent_fn.__name__, self.stage_name), loss_xent_meters.mean, epoch)\n",
    "            writer.add_scalar(\"{}/{}\".format(self.loss_mse_fn.__name__, self.stage_name), loss_mse_meters.mean, epoch)\n",
    "        return logs\n",
    "    \n",
    "def get_tensor_onehot(y, num_class = 2):\n",
    "    N = len(y)\n",
    "    onehot = torch.zeros((N, num_class), dtype=torch.float32, device=y.device)\n",
    "    onehot = onehot.scatter_(1, y.unsqueeze(1), 1)\n",
    "    return onehot \n",
    "\n",
    "def mixup(x1, x2, y1, y2, alpha):\n",
    "    beta = np.random.beta(alpha, alpha)\n",
    "    beta = max(beta, 1-beta)\n",
    "    beta = torch.tensor(beta, dtype=x1.dtype, device = x1.device)\n",
    "    x = beta * x1 + (1 - beta) * x2\n",
    "    y = beta * y1 + (1 - beta) * y2\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftLabelCrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    @property\n",
    "    def __name__(self):\n",
    "        return 'CrossEntropyLoss'\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_pred: Variable :math:`(N, C)` where `C = number of classes`\n",
    "            y_true: Variable :math:`(N, C)` where each value is torch.Floattensor\n",
    "        Returns:\n",
    "            softmax-cross_entropy\n",
    "        \"\"\"\n",
    "        log_likelihood = -1*nn.LogSoftmax(dim=1)(y_pred)\n",
    "        N, C = y_pred.size()\n",
    "        loss = torch.sum(torch.mul(log_likelihood, y_true))/N\n",
    "        return loss\n",
    "    \n",
    "class MSELoss(nn.Module):\n",
    "    def __init__(self, max_value = 1):\n",
    "        super().__init__()\n",
    "        self.max_value = max_value\n",
    "    @property\n",
    "    def __name__(self):\n",
    "        return 'mseloss'\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_pred: Variable :math:`(N, C)` where `C = number of classes`\n",
    "            y_true: Variable :math:`(N, C)` where each value is torch.Floattensor\n",
    "        Returns:\n",
    "            softmax-cross_entropy\n",
    "        \"\"\"\n",
    "        return torch.nn.MSELoss()(y_pred, y_true)\n",
    "    \n",
    "class SoftmaxMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    @property\n",
    "    def __name__(self):\n",
    "        return \"SoftmaxMSELoss\"\n",
    "    def forward(self, y_pred, y_gt):\n",
    "        y_pred = F.softmax(y_pred, dim = 1)\n",
    "        y_gt = F.softmax(y_gt, dim = 1)\n",
    "        return nn.MSELoss()(y_pred, y_gt) \n",
    "    \n",
    "class CrossEntropy(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.kwargs = kwargs\n",
    "    @property\n",
    "    def __name__(self):\n",
    "        return 'CrossEntropyLoss'\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_pred: Variable :math:`(N, C)` where `C = number of classes`\n",
    "            y_true: Variable :math:`(N)` where each value is `0 <= targets[i] <= C-1`, torch.Longtensor\n",
    "        Returns:\n",
    "            softmax-cross_entropy\n",
    "        \"\"\"\n",
    "        return nn.CrossEntropyLoss(**self.kwargs)(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "# model.load_state_dict(torch.load(\"_ckpt_epoch_9.h5\"))\n",
    "model.cuda()\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone   \n",
    "\n",
    "# Initail predict running average\n",
    "predict_running_average = {}\n",
    "for p in train_images:\n",
    "    predict_running_average[p] = 0\n",
    "\n",
    "taipei = timezone('Asia/Taipei')\n",
    "taipei_time = datetime.now(taipei)\n",
    "current_time = taipei_time.strftime('%Y-%m-%d_%H-%M')\n",
    "\n",
    "loss_fn = utils.global_objective.AUCPRHingeLoss()\n",
    "loss_xent_fn = SoftLabelCrossEntropy()\n",
    "# loss_xent_fn = utils.global_objective.AUCPRHingeLoss()\n",
    "loss_mse_fn = MSELoss()\n",
    "filter_loss_fn = CrossEntropy(reduction=\"none\")\n",
    "\n",
    "metrics = [utils.metrics.Fscore()]\n",
    "\n",
    "optimizer = torch.optim.SGD([ \n",
    "    dict(params=model.parameters(), lr=1e-4),\n",
    "], weight_decay=1e-3,  momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup = 5\n",
    "batch_size = 128\n",
    "epochs_in_iteration = 2\n",
    "max_score = 0.8\n",
    "max_iteration = 10\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# model_name = current_time + \"_Retrain_MixMatch_ResNet18_ensemble_GMM\"\n",
    "model_name = \"2021-02-02_10-59_Retrain_MixMatch_ResNet18_ensemble_GMM\"\n",
    "log_folder_name = os.path.join('/data/log_folder/mixmatch/',model_name)\n",
    "\n",
    "# Writer\n",
    "writer = SummaryWriter(log_dir=log_folder_name, flush_secs=3)\n",
    "print(log_folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initial train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LabeledDataset(\n",
    "    patches = train_images,\n",
    "    patch_cls_lookup = train_lookup,\n",
    "    augmentation = get_training_augmentation(),\n",
    "    preprocessing = get_preprocessing(),\n",
    ")\n",
    "\n",
    "valid_dataset = LabeledDataset(\n",
    "    patches = valid_images,\n",
    "    patch_cls_lookup = valid_lookup,\n",
    "    augmentation = get_validation_augmentation(),\n",
    "    preprocessing = get_preprocessing(),\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True, num_workers=16, pin_memory=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n#=====================#')\n",
    "print('∥   Initial training  ∥')\n",
    "print('#=====================#')\n",
    "train_epoch = InitialTrainEpoch(\n",
    "    model, \n",
    "    loss_fn,\n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "valid_epoch = ValidEpoch(\n",
    "    model,\n",
    "    loss_fn,\n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "train_epoch.stage_name = 'Initial train'\n",
    "valid_epoch.stage_name = 'Initial valid'\n",
    "\n",
    "for i in range(warmup, 5):\n",
    "    train_logs = train_epoch.run(train_loader, i)\n",
    "    valid_logs = valid_epoch.run(valid_loader, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/data/weight/noisy_label/{}_epoch_{}_auc_{:.4f}.h5'.format(\n",
    "    model_name, 5, valid_logs['auc'])\n",
    "torch.save(model.state_dict(), output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/data/weight/noisy_label/2021-02-02_10-59_Retrain_MixMatch_ResNet18_ensemble_GMM_epoch_5_auc_0.8582.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([ \n",
    "    dict(params=model.parameters(), lr=1e-4),\n",
    "], weight_decay=1e-3,  momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_epoch = NewFilterEpoch(\n",
    "        model, \n",
    "        init_lookup=train_lookup,\n",
    "        loss=filter_loss_fn,\n",
    "        metrics=metrics,\n",
    "        ensemble_momentum=0.5,\n",
    "        device=DEVICE,\n",
    "        verbose=True,\n",
    "    )\n",
    "train_epoch = TrainEpoch(\n",
    "        model,\n",
    "        loss_xent_fn, \n",
    "        loss_mse_fn, \n",
    "        optimizer, \n",
    "        device=DEVICE,\n",
    "        verbose=True,\n",
    "    )\n",
    "valid_epoch = ValidEpoch(\n",
    "    model,\n",
    "    loss_fn,\n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "while(iteration < 20):\n",
    "    \"\"\"\n",
    "    Filter label\n",
    "    \"\"\"\n",
    "    print('\\n------------------------------------------------------------')\n",
    "    print('Iteration: {}\\n#=====================#'.format(iteration))\n",
    "    print('∥   Start Filtering   ∥')\n",
    "    print('#=====================#')\n",
    "    \n",
    "    filtered_label = filter_epoch.run(train_npys, iteration)\n",
    "    # apply new label\n",
    "    labeled_patches, unlabeled_patches, num_p= split_labeled_unlabeled(train_filtered_label = filtered_label)\n",
    "    print('\\n# of Labeled :{}, # of Unlabeled :{}'.format(len(labeled_patches), len(unlabeled_patches)))\n",
    "    print('# of Positive Labeled :{}, # of Negative Labeled :{}'.format(num_p, len(labeled_patches) - num_p))\n",
    "    \n",
    "    labeled_dataset = LabeledDataset(\n",
    "        patches = labeled_patches,\n",
    "        patch_cls_lookup = filtered_label,\n",
    "        augmentation = get_training_augmentation(),\n",
    "        preprocessing = get_preprocessing(),\n",
    "    )\n",
    "    unlabeled_dataset = UnlabeledDataset(\n",
    "        patches = unlabeled_patches,\n",
    "        augmentation = get_training_augmentation(),\n",
    "        preprocessing = get_preprocessing(),\n",
    "    )\n",
    "    labeled_loader = DataLoader(labeled_dataset, batch_size=batch_size//2, \\\n",
    "                                num_workers=10, shuffle = True, pin_memory=True, drop_last=True)\n",
    "    unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=batch_size//2, \\\n",
    "                                  num_workers=10, shuffle = True, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    \"\"\"\n",
    "    Model train\n",
    "    \"\"\" \n",
    "    print('\\n#=====================#')\n",
    "    print('∥    Start training   ∥')\n",
    "    print('#=====================#')\n",
    "    \n",
    "    train_epoch.stage_name = 'Iteration train'\n",
    "    valid_epoch.stage_name = 'Iteration valid'\n",
    "    \n",
    "    #train with new label\n",
    "    for i in range(epochs_in_iteration):\n",
    "        epoch = iteration*epochs_in_iteration+i\n",
    "        print('\\nEpoch: {}, batch: {}'.format(epoch, batch_size))\n",
    "        print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "        train_logs = train_epoch.run(labeled_loader, unlabeled_loader, epoch)\n",
    "        valid_logs = valid_epoch.run(valid_loader, epoch)\n",
    "        \n",
    "        if max_score < valid_logs['auc']:\n",
    "            max_score = valid_logs['auc']\n",
    "        \n",
    "            output_path = '/data/weight/noisy_label/{}_epoch_{}_auc_{:.4f}.h5'.format(\n",
    "                model_name, epochs_in_iteration, valid_logs['auc'])\n",
    "            torch.save(model.state_dict(), output_path)\n",
    "            print('Model saved! {}'.format(output_path))    \n",
    "    \n",
    "    if iteration % 3 == 0:\n",
    "        \n",
    "        output_path = '/data/weight/noisy_label/{}_epoch_{}_auc_{:.4f}.h5'.format(\n",
    "            model_name, epochs_in_iteration, valid_logs['auc'])\n",
    "        torch.save(model.state_dict(), output_path)\n",
    "        print('Model saved! {}'.format(output_path)) \n",
    "    \n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using_npy = \"/data/tcga/kmeans_cluster_32/\"\n",
    "using_npy = \"/data/tcga/512denseTumor/\"\n",
    "valid_npy_pos = []\n",
    "valid_npy_neg = []\n",
    "for npy in sorted(glob.glob(os.path.join(using_npy, \"*.npy\"))):    \n",
    "    patient = npy.split(\"/\")[-1][:12]\n",
    "    if patient in valid_patient:\n",
    "        if lookup[patient] == 1:\n",
    "            valid_npy_pos.append(npy)\n",
    "        else:\n",
    "            valid_npy_neg.append(npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = \"/data/weight/noisy_label/2020-12-29_20-57_MixMatch_ResNet18_ensemblePred_epoch_0_auc_0.8328042328042329.h5\"\n",
    "# weight = \"/data/weight/noisy_label/2021-01-08_16-17_MixMatch_ResNet18_ensemble+GMM_epoch_1_auc_0.8006.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = models.resnet18()\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model.load_state_dict(torch.load(weight))\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TT_positive_pred = {}\n",
    "with tqdm(valid_npy_pos, desc=\"test\", file=sys.stdout) as iterator:\n",
    "    for npy in iterator:\n",
    "        svs_name = npy.split(\"/\")[-1][:-4]\n",
    "        patient_name = svs_name[:12]\n",
    "        x_y_pairs = np.load(npy)\n",
    "        image_names = [\"{}_{}_{}.jpg\".format(svs_name, x, y) for x, y in x_y_pairs]\n",
    "        \n",
    "        test_dataset = CustomDataset(\n",
    "            image_names,\n",
    "            augmentation=get_validation_augmentation(),\n",
    "            preprocessing=get_preprocessing()\n",
    "        )\n",
    "        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=16, pin_memory=True)\n",
    "        patch_predictions = np.array([])\n",
    "        svs_pred = [0,0]\n",
    "        for images, labels, patch_names in test_loader:\n",
    "            with torch.no_grad():\n",
    "                outputs = model.forward(images.cuda())\n",
    "            outputs = torch.softmax(outputs, dim=1)\n",
    "            outputs = outputs[:, 1].detach().cpu().numpy()\n",
    "            patch_predictions = np.concatenate((patch_predictions, outputs))\n",
    "            for p in outputs:\n",
    "                if p > 0.5:\n",
    "                    svs_pred[1] += 1\n",
    "                else:\n",
    "                    svs_pred[0] += 1\n",
    "        y_pred = svs_pred[1]/(svs_pred[0]+svs_pred[1])\n",
    "        if patient_name not in TT_positive_pred:\n",
    "            TT_positive_pred[patient_name] = []\n",
    "        TT_positive_pred[patient_name].append(y_pred)\n",
    "        \n",
    "        title = \"{}_gt={}_pred={:.4f}_raw={}\".format(svs_name, lookup[patient_name], y_pred, raw_TMB_dict[patient_name])\n",
    "        data_process.stitch.stitch(wsi_name = svs_name, x_y_pairs = x_y_pairs, preds = patch_predictions, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TT_negative_pred = {}\n",
    "with tqdm(valid_npy_neg, desc=\"test\", file=sys.stdout) as iterator:\n",
    "    for npy in iterator:\n",
    "        svs_name = npy.split(\"/\")[-1][:-4]\n",
    "        patient_name = svs_name[:12]\n",
    "        x_y_pairs = np.load(npy)\n",
    "        image_names = [\"{}_{}_{}.jpg\".format(svs_name, x, y) for x, y in x_y_pairs]\n",
    "        \n",
    "        test_dataset = CustomDataset(\n",
    "            image_names,\n",
    "            augmentation=get_validation_augmentation(),\n",
    "            preprocessing=get_preprocessing()\n",
    "        )\n",
    "        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=16, pin_memory=True)\n",
    "        patch_predictions = np.array([])\n",
    "        svs_pred = [0,0]\n",
    "        for images, labels, patch_names in test_loader:\n",
    "            with torch.no_grad():\n",
    "                outputs = model.forward(images.cuda())\n",
    "            outputs = torch.softmax(outputs, dim=1)\n",
    "            outputs = outputs[:, 1].detach().cpu().numpy()\n",
    "            patch_predictions = np.concatenate((patch_predictions, outputs))\n",
    "            for p in outputs:\n",
    "                if p > 0.5:\n",
    "                    svs_pred[1] += 1\n",
    "                else:\n",
    "                    svs_pred[0] += 1\n",
    "        y_pred = svs_pred[1]/(svs_pred[0]+svs_pred[1])\n",
    "        if patient_name not in TT_negative_pred:\n",
    "            TT_negative_pred[patient_name] = []\n",
    "        TT_negative_pred[patient_name].append(y_pred)\n",
    "        \n",
    "        title = \"{}_gt={}_pred={:.4f}_raw={}\".format(svs_name, lookup[patient_name], y_pred, raw_TMB_dict[patient_name])\n",
    "        data_process.stitch.stitch(wsi_name = svs_name, x_y_pairs = x_y_pairs, preds = patch_predictions, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "preds = [np.amax(values) for key, values in TT_positive_pred.items()] \\\n",
    "        + [np.amax(values) for key, values in TT_negative_pred.items()]\n",
    "gt = [1]*len(TT_positive_pred) + [0]*len(TT_negative_pred)\n",
    "print(sklearn.metrics.roc_auc_score(gt, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/tcga/cohort_count.pkl\", \"rb\") as fp:\n",
    "    raw_TMB_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch(wsi_name = None, edge_resize_factor = 32, tile_size = 512, overlap = 256, x_y_pairs = None, preds = None, title = \"\"):\n",
    "    import pickle\n",
    "    ORIGINAL_FOLDER = \"/data/tcga/svs/masks\"\n",
    "    original_image = data_process.wsi_utils.vips_get_image(os.path.join(ORIGINAL_FOLDER, svs_name[:-4]+\".png\"))\n",
    "    \n",
    "    with open(\"./svs_x_y.pkl\", \"rb\") as fp:\n",
    "        svs_x_y_dict = pickle.load(fp)\n",
    "    width, height = svs_x_y_dict[wsi_name]\n",
    "    tumor_mask = np.zeros((int(height/edge_resize_factor), int(width/edge_resize_factor))).astype('float32')\n",
    "\n",
    "    for xy, pred in zip(x_y_pairs, preds):\n",
    "        x, y = xy\n",
    "        start_x = int(x/edge_resize_factor)\n",
    "        end_x = int((x+tile_size)/edge_resize_factor)\n",
    "        start_y = int(y/edge_resize_factor)\n",
    "        end_y = int((y+tile_size)/edge_resize_factor)\n",
    "        tumor_mask[start_y:end_y, start_x:end_x] = pred\n",
    "    \n",
    "    tumor_mask[tumor_mask == 0.0] = np.nan\n",
    "    \"\"\" plt heatmap \"\"\"\n",
    "#     fig = plt.figure(figsize=(15, 30))\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "#     plt.suptitle(title,fontsize=15, y=0.8)\n",
    "    # ax = plt.axes()\n",
    "#     ax = plt.subplot(121)\n",
    "    ax[0].title.set_text(title)\n",
    "    im = ax[0].imshow(tumor_mask, cmap = 'coolwarm', vmin=0, vmax=1)\n",
    "    cax = fig.add_axes([ax[0].get_position().x1+0.01,ax[0].get_position().y0,0.02,ax[0].get_position().height])\n",
    "    plt.colorbar(im, cax=cax) # Similar to fig.colorbar(im, cax = cax)\n",
    "    \"\"\" plt original image \"\"\"\n",
    "#     ax2 = plt.subplot(122)\n",
    "    im = ax[1].imshow(original_image, vmin=0, vmax=255)\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, patches, augmentation = None, preprocessing = None):\n",
    "        self.patches = patches\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        image_name = self.patches[i]\n",
    "        svs_name = image_name.split(\"_\")[0]\n",
    "        full_path = os.path.join(IMAGE_FOLDER, svs_name, image_name)\n",
    "        image = data_process.wsi_utils.vips_get_image(full_path)\n",
    "        \n",
    "        cls = lookup[image_name[:12]]\n",
    "        \n",
    "        if(self.augmentation):\n",
    "            sampled = self.augmentation(image = image)\n",
    "            _input = sampled['image']\n",
    "            \n",
    "        if(self.preprocessing):\n",
    "            sampled = self.preprocessing(image = _input)\n",
    "            _input = sampled['image']\n",
    "            \n",
    "        return _input, cls, image_name\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
