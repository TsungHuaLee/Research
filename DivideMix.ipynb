{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as albu\n",
    "import os, glob, sys, shutil, random\n",
    "import cv2, itertools, random, pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_process\n",
    "import utils\n",
    "IMAGE_FOLDER = \"/data/tcga/512dense/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed_all(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cohort: 389\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"./cohort_high_low.pkl\", \"rb\") as fp:\n",
    "    cohort_count_dict = pickle.load(fp)\n",
    "print(\"# cohort: {}\".format(len(cohort_count_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 317, 1: 72})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_ids = list(cohort_count_dict.keys())\n",
    "patient_cls = list(cohort_count_dict.values())\n",
    "lookup = dict(zip(patient_ids, patient_cls))\n",
    "Counter(patient_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train patient:Counter({0: 221, 1: 51})\n",
      "# valid patient:Counter({0: 96, 1: 21})\n"
     ]
    }
   ],
   "source": [
    "train_patient, valid_patient = train_test_split(patient_ids, test_size = 0.3, random_state = 42)\n",
    "train_cls = [lookup[i] for i in train_patient]\n",
    "valid_cls = [lookup[i] for i in valid_patient]\n",
    "print(\"# train patient:{}\\n# valid patient:{}\".format(Counter(train_cls), Counter(valid_cls)))\n",
    "del train_cls, valid_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train images:1014641\n",
      "# valid images:448232\n",
      "# train images:Counter({0: 809418, 1: 205223})\n",
      "# valid images:Counter({0: 345166, 1: 103066})\n",
      "# train npys:Counter({396: 1, 97: 1})\n",
      "# valid npys:Counter({181: 1, 41: 1})\n"
     ]
    }
   ],
   "source": [
    "train_images, valid_images = [], []\n",
    "train_lookup, valid_lookup = {}, {}\n",
    "train_npys = []\n",
    "train_p_n, valid_p_n = [0, 0], [0, 0]\n",
    "\n",
    "for idx, npy in enumerate(sorted(glob.glob(\"/data/tcga/512denseTumor/*.npy\"))):\n",
    "    if npy == \"/data/tcga/512denseTumor/TCGA-CM-6679-01A-01-TS1.6fbabb32-470f-4fc2-9a73-3a40e1237988.npy\":\n",
    "        continue\n",
    "    x_y_pairs = np.load(npy)\n",
    "    svs_name = npy.split(\"/\")[-1][:-4]\n",
    "    patient = svs_name[:12]\n",
    "    for x, y in x_y_pairs:\n",
    "        path = \"{}_{}_{}.jpg\".format(svs_name, x, y)\n",
    "        if patient in train_patient:\n",
    "            train_images.append(path)\n",
    "            train_lookup[path[:-4]] = lookup[patient]\n",
    "        else:\n",
    "            valid_images.append(path)\n",
    "            valid_lookup[path[:-4]] = lookup[patient]\n",
    "    \n",
    "    if patient in train_patient:\n",
    "        train_npys.append(npy)\n",
    "        train_p_n[lookup[patient]] += 1\n",
    "    if patient in valid_patient:\n",
    "        valid_p_n[lookup[patient]] += 1\n",
    "#     \"\"\" for demo \"\"\"\n",
    "#     if idx > 70:\n",
    "#         break\n",
    "train_images = np.array(train_images)\n",
    "valid_images = np.array(valid_images)\n",
    "print(\"# train images:{}\\n# valid images:{}\".format(len(train_images), len(valid_images)))\n",
    "print(\"# train images:{}\\n# valid images:{}\".format(Counter(list(train_lookup.values())), \\\n",
    "                                      Counter(list(valid_lookup.values())) ))\n",
    "print(\"# train npys:(0: {}, 1: {})\\n# valid npys:(0: {}, 1: {})\".format(train_p_n[0], train_p_n[1], \\\n",
    "                                      valid_p_n[0], valid_p_n[1]))\n",
    "del train_p_n, valid_p_n\n",
    "del train_p_n, valid_p_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, **kwargs):\n",
    "    x = x/255.\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def get_preprocessing():\n",
    "    _transform = [\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)\n",
    "\n",
    "def get_training_augmentation():\n",
    "    test_transform = [\n",
    "        albu.Resize(224, 224),\n",
    "        albu.Flip(),\n",
    "        albu.RandomRotate90(),\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.ElasticTransform(p=1),\n",
    "                albu.GridDistortion(p=1),\n",
    "                albu.OpticalDistortion(p=1)\n",
    "            ],\n",
    "            p=0.8,\n",
    "        ),\n",
    "        albu.ShiftScaleRotate(border_mode=0, value=0),\n",
    "        albu.IAAAdditiveGaussianNoise(),\n",
    "        albu.GaussianBlur(),\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.RandomBrightnessContrast(p=1),\n",
    "                albu.HueSaturationValue(p=1),\n",
    "                albu.HueSaturationValue(hue_shift_limit=40, sat_shift_limit=60, val_shift_limit=40, p=1),\n",
    "            ], \n",
    "            p=0.8,\n",
    "        ),\n",
    "        albu.ColorJitter(p=0.5)\n",
    "        \n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    test_transform = [\n",
    "        albu.Resize(224, 224),\n",
    "    ]\n",
    "    return albu.Compose(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): \n",
    "    def __init__(self, root_dir, mode, images = None, lookup_table = None, \n",
    "                 augmentation = None, preprocessing = None, pred=[], probability=[], log=''): \n",
    "        self.root = root_dir\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.labels = lookup_table\n",
    "\n",
    "        if self.mode == 'all':\n",
    "            self.images = images                \n",
    "        elif self.mode == \"labeled\":\n",
    "            pred_idx = pred.nonzero()[0]\n",
    "            self.images = [images[i] for i in pred_idx]                \n",
    "            self.probability = [probability[i] for i in pred_idx]            \n",
    "            print(\"%s data has a size of %d\"%(self.mode,len(self.images)))            \n",
    "            log.write('Numer of labeled samples:%d \\n'%(pred.sum()))\n",
    "            log.flush()                          \n",
    "        elif self.mode == \"unlabeled\":\n",
    "            pred_idx = (1-pred).nonzero()[0]                                               \n",
    "            self.images = [images[i] for i in pred_idx]                           \n",
    "            print(\"%s data has a size of %d\"%(self.mode,len(self.images)))\n",
    "        elif self.mode == \"test\":\n",
    "            self.images = images\n",
    "                    \n",
    "    def __getitem__(self, index):\n",
    "        if self.mode=='labeled':\n",
    "            img_path = self.images[index]\n",
    "            svs_name = img_path.split(\"_\")[0]\n",
    "            target = self.labels[img_path[:-4]] \n",
    "            prob = self.probability[index]\n",
    "            \n",
    "            full_path = os.path.join(self.root, svs_name, img_path)\n",
    "            image = data_process.wsi_utils.vips_get_image(full_path)\n",
    "            \n",
    "            img1 = self.augmentation(image = image)['image']\n",
    "            img2 = self.augmentation(image = image)['image']\n",
    "            img1 = self.preprocessing(image = img1)['image']\n",
    "            img2 = self.preprocessing(image = img2)['image']\n",
    "            return img1, img2, target, prob              \n",
    "        elif self.mode=='unlabeled':\n",
    "            img_path = self.images[index]\n",
    "            svs_name = img_path.split(\"_\")[0]\n",
    "            full_path = os.path.join(self.root, svs_name, img_path)\n",
    "            image = data_process.wsi_utils.vips_get_image(full_path)\n",
    "            \n",
    "            img1 = self.augmentation(image = image)['image']\n",
    "            img2 = self.augmentation(image = image)['image']\n",
    "            img1 = self.preprocessing(image = img1)['image']\n",
    "            img2 = self.preprocessing(image = img2)['image']\n",
    "            return img1, img2  \n",
    "        elif self.mode=='all':\n",
    "            img_path = self.images[index]\n",
    "            svs_name = img_path.split(\"_\")[0]\n",
    "            target = self.labels[img_path[:-4]] \n",
    "            \n",
    "            full_path = os.path.join(self.root, svs_name, img_path)\n",
    "            image = data_process.wsi_utils.vips_get_image(full_path)  \n",
    "            \n",
    "            img = self.augmentation(image = image)['image']\n",
    "            img = self.preprocessing(image = img)['image']\n",
    "            return img, target, index\n",
    "        elif self.mode=='test':\n",
    "            img_path = self.images[index]\n",
    "            svs_name = img_path.split(\"_\")[0]\n",
    "            target = self.labels[img_path[:-4]] \n",
    "            \n",
    "            full_path = os.path.join(IMAGE_FOLDER, svs_name, img_path)\n",
    "            image = data_process.wsi_utils.vips_get_image(full_path)\n",
    "\n",
    "            img = self.augmentation(image = image)['image']\n",
    "            img = self.preprocessing(image = img)['image']\n",
    "            return image, cls, img_path[:-4]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "class WrapperLoader():  \n",
    "    def __init__(self, batch_size, num_workers, log):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.root_dir = IMAGE_FOLDER\n",
    "        self.log = log\n",
    "\n",
    "    def run(self,mode,pred=[],prob=[]):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        if mode=='warmup':\n",
    "            all_dataset = CustomDataset(root_dir=self.root_dir, mode=\"all\", images=train_images, lookup_table=train_lookup,\n",
    "                                            augmentation=get_training_augmentation(), preprocessing=get_preprocessing())                \n",
    "            trainloader = DataLoader(\n",
    "                dataset=all_dataset, \n",
    "                batch_size=self.batch_size*2,\n",
    "                shuffle=True,\n",
    "                num_workers=self.num_workers,\n",
    "                pin_memory=True)                 \n",
    "            return trainloader\n",
    "                                     \n",
    "        elif mode=='train':\n",
    "            labeled_dataset = CustomDataset(root_dir=self.root_dir, mode=\"labeled\", pred=pred, probability=prob, log=self.log, \n",
    "                                                images=train_images, lookup_table=train_lookup,\n",
    "                                                augmentation=get_training_augmentation(), preprocessing=get_preprocessing())            \n",
    "            labeled_trainloader = DataLoader(\n",
    "                dataset=labeled_dataset, \n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=self.num_workers,\n",
    "                pin_memory=True)        \n",
    "            \n",
    "            unlabeled_dataset =  CustomDataset(root_dir=self.root_dir, mode=\"unlabeled\", pred=pred, log=self.log, \n",
    "                                                images=train_images, lookup_table=train_lookup,\n",
    "                                                augmentation=get_training_augmentation(), preprocessing=get_preprocessing())                   \n",
    "            unlabeled_trainloader = DataLoader(\n",
    "                dataset=unlabeled_dataset, \n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=self.num_workers,\n",
    "                pin_memory=True)\n",
    "            return labeled_trainloader, unlabeled_trainloader\n",
    "       \n",
    "        elif mode=='eval_train':\n",
    "            eval_dataset = CustomDataset(root_dir=self.root_dir, mode=\"all\", images=train_images, lookup_table=train_lookup,\n",
    "                                            augmentation=get_training_augmentation(), preprocessing=get_preprocessing())    \n",
    "            eval_loader = DataLoader(\n",
    "                dataset=eval_dataset, \n",
    "                batch_size=self.batch_size*5,\n",
    "                shuffle=False,\n",
    "                num_workers=self.num_workers,\n",
    "                pin_memory=True)               \n",
    "            return eval_loader\n",
    "        \n",
    "        elif mode==\"test\":\n",
    "            test_dataset = CustomDataset(root_dir=self.root_dir, mode=\"all\", images=valid_images, lookup_table=valid_lookup,\n",
    "                                            augmentation=get_training_augmentation(), preprocessing=get_preprocessing()) \n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, \n",
    "                batch_size=self.batch_size*5, \n",
    "                shuffle=False, \n",
    "                num_workers=self.num_workers, \n",
    "                pin_memory=True)\n",
    "            return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dividemix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParam = {\n",
    "    \"num_class\": 2,\n",
    "    \"T\": 0.5,\n",
    "    \"alpha\": 4,\n",
    "    \"lambda_u\": 20,\n",
    "    \"p_threshold\": 0.5 # clean label threshold tao\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch,net,net2,optimizer,labeled_trainloader,unlabeled_trainloader):\n",
    "    net.train()\n",
    "    net2.eval() #fix one network and train the other\n",
    "    \n",
    "    unlabeled_train_iter = iter(unlabeled_trainloader)    \n",
    "    num_iter = (len(labeled_trainloader.dataset)//bs)+1\n",
    "    \n",
    "    labeled_loss_meter = utils.meter.AverageValueMeter()\n",
    "    unlabeled_loss_meter = utils.meter.AverageValueMeter()\n",
    "    total_loss_meter = utils.meter.AverageValueMeter()\n",
    "    \n",
    "    with tqdm(labeled_trainloader, desc=\"train\", file=sys.stdout) as iterator:\n",
    "        for batch_idx, (inputs_x, inputs_x2, labels_x, w_x) in enumerate(iterator):      \n",
    "            try:\n",
    "                inputs_u, inputs_u2 = unlabeled_train_iter.next()\n",
    "            except:\n",
    "                unlabeled_train_iter = iter(unlabeled_trainloader)\n",
    "                inputs_u, inputs_u2 = unlabeled_train_iter.next()                 \n",
    "            batch_size = inputs_x.size(0)\n",
    "\n",
    "            # Transform label to one-hot\n",
    "            labels_x = torch.zeros(batch_size, hyperParam[\"num_class\"]).scatter_(1, labels_x.view(-1,1), 1)        \n",
    "            w_x = w_x.view(-1,1).type(torch.FloatTensor) \n",
    "\n",
    "            inputs_x, inputs_x2, labels_x, w_x = inputs_x.cuda(), inputs_x2.cuda(), labels_x.cuda(), w_x.cuda()\n",
    "            inputs_u, inputs_u2 = inputs_u.cuda(), inputs_u2.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # label co-guessing of unlabeled samples\n",
    "                outputs_u11 = net(inputs_u)\n",
    "                outputs_u12 = net(inputs_u2)\n",
    "                outputs_u21 = net2(inputs_u)\n",
    "                outputs_u22 = net2(inputs_u2)            \n",
    "                pu = (torch.softmax(outputs_u11, dim=1) + torch.softmax(outputs_u12, dim=1) \\\n",
    "                      + torch.softmax(outputs_u21, dim=1) + torch.softmax(outputs_u22, dim=1)) / 4       \n",
    "                ptu = pu**(1/hyperParam[\"T\"]) # temparature sharpening\n",
    "                \n",
    "                targets_u = ptu / ptu.sum(dim=1, keepdim=True) # normalize\n",
    "                targets_u = targets_u.detach()       \n",
    "                # label refinement of labeled samples\n",
    "                outputs_x = net(inputs_x)\n",
    "                outputs_x2 = net(inputs_x2)            \n",
    "\n",
    "                px = (torch.softmax(outputs_x, dim=1) + torch.softmax(outputs_x2, dim=1)) / 2\n",
    "                px = w_x*labels_x + (1-w_x)*px              \n",
    "                ptx = px**(1/hyperParam[\"T\"]) # temparature sharpening \n",
    "\n",
    "                targets_x = ptx / ptx.sum(dim=1, keepdim=True) # normalize           \n",
    "                targets_x = targets_x.detach()       \n",
    "\n",
    "            # mixmatch\n",
    "            l = np.random.beta(hyperParam[\"alpha\"], hyperParam[\"alpha\"])        \n",
    "            l = max(l, 1-l)\n",
    "\n",
    "            all_inputs = torch.cat([inputs_x, inputs_x2, inputs_u, inputs_u2], dim=0)\n",
    "            all_targets = torch.cat([targets_x, targets_x, targets_u, targets_u], dim=0)\n",
    "\n",
    "            idx = torch.randperm(all_inputs.size(0))\n",
    "\n",
    "            input_a, input_b = all_inputs, all_inputs[idx]\n",
    "            target_a, target_b = all_targets, all_targets[idx]\n",
    "\n",
    "            mixed_input = l * input_a + (1 - l) * input_b        \n",
    "            mixed_target = l * target_a + (1 - l) * target_b\n",
    "\n",
    "            logits = net(mixed_input)\n",
    "            logits_x = logits[:batch_size*2]\n",
    "            logits_u = logits[batch_size*2:]        \n",
    "\n",
    "            Lx, Lu, lamb = criterion(logits_x, mixed_target[:batch_size*2], \\\n",
    "                                     logits_u, mixed_target[batch_size*2:], \\\n",
    "                                     epoch+batch_idx/num_iter, warm_up)\n",
    "            \n",
    "            # regularization\n",
    "            prior = torch.ones(hyperParam[\"num_class\"])/hyperParam[\"num_class\"]\n",
    "            prior = prior.cuda()        \n",
    "            pred_mean = torch.softmax(logits, dim=1).mean(0)\n",
    "            penalty = torch.sum(prior*torch.log(prior/pred_mean))\n",
    "\n",
    "            loss = Lx + lamb * Lu  + penalty\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            labeled_loss_meter.add(Lx.item())\n",
    "            unlabeled_loss_meter.add(Lu.item())\n",
    "            total_loss_meter.add(loss.item())\n",
    "            \n",
    "            s = 'Labeled loss: %.4f, Unlabeled loss:%.4f, Total loss: %.4f'\\\n",
    "                %(labeled_loss_meter.mean, unlabeled_loss_meter.mean, total_loss_meter.mean)\n",
    "            iterator.set_postfix_str(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup(epoch,net,optimizer,dataloader):\n",
    "    noise_mode = 'sym'\n",
    "    net.train()\n",
    "    \n",
    "    loss_meter = utils.meter.AverageValueMeter()\n",
    "    penalty_loss_meter = utils.meter.AverageValueMeter()\n",
    "    ce_loss_meter = utils.meter.AverageValueMeter()\n",
    "    with tqdm(dataloader, desc=\"warmup\", file=sys.stdout) as iterator:\n",
    "        for batch_idx, (inputs, labels, path) in enumerate(iterator):      \n",
    "            inputs, labels = inputs.cuda(), labels.cuda() \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)               \n",
    "            loss = CEloss(outputs, labels)      \n",
    "            if noise_mode=='asym':  # penalize confident prediction for asymmetric noise\n",
    "#                 penalty = conf_penalty(outputs)\n",
    "                penalty = NegEntropy()(outputs)\n",
    "                L = loss - penalty\n",
    "            elif noise_mode=='sym':   \n",
    "                L = loss\n",
    "            L.backward()  \n",
    "            optimizer.step()\n",
    "            \n",
    "            if noise_mode=='sym':\n",
    "                loss_meter.add(L.item())\n",
    "                s = 'CE-loss: %.4f'%(loss_meter.mean)\n",
    "                iterator.set_postfix_str(s)\n",
    "            elif noise_mode=='asym':\n",
    "                loss_meter.add(L.item())\n",
    "                penalty_loss_meter.add(penalty.item())\n",
    "                ce_loss_meter.add(loss.item())\n",
    "                s = \"Total-loss: {:.4f}, CE-loss: {:.4f}, Penalty: {:.4f}\".format(loss_meter.mean, ce_loss_meter.mean, penalty_loss_meter.mean)\n",
    "                iterator.set_postfix_str(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def test(epoch,net1,net2):\n",
    "    net1.eval()\n",
    "    net2.eval()\n",
    "    patient_preds = {}\n",
    "    \n",
    "    loss_meter = utils.meter.AverageValueMeter()\n",
    "    metric_meter = utils.meter.AverageValueMeter()\n",
    "    with tqdm(test_loader, desc=\"valid\", file=sys.stdout) as iterator:\n",
    "        for inputs, targets, names in iterator:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            with torch.no_grad():\n",
    "                outputs1 = net1(inputs)\n",
    "                outputs2 = net2(inputs)\n",
    "            outputs = outputs1+outputs2\n",
    "            \n",
    "            loss_value = CEloss(outputs, targets)\n",
    "            loss_value = loss_value.detach().cpu().numpy()\n",
    "            metric_value = utils.metrics.Fscore()(outputs, targets)\n",
    "            metric_value = metric_value.detach().cpu().numpy()\n",
    "            loss_meter.add(loss_value)\n",
    "            metric_meter.add(metric_value)\n",
    "            \n",
    "            _, outputs = torch.max(outputs, 1)  \n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            for pid, p in zip(names, outputs):\n",
    "                pid = pid[:12]\n",
    "                if pid not in patient_preds:\n",
    "                    patient_preds[pid] = [0,0]\n",
    "                patient_preds[pid][p] += 1\n",
    "            s = \"{}-{:.2f}, {}-{:.2f}\".format(\"CE-loss\", loss_meter.mean,\"fscore\", metric_meter.mean)\n",
    "            iterator.set_postfix_str(s)\n",
    "            \n",
    "    y_pred, y_gt = [], []\n",
    "    for key, values in patient_preds.items():\n",
    "        y_gt.append(lookup[key])\n",
    "        y_pred.append(values[1]/(values[0]+values[1]))\n",
    "    auc = sklearn.metrics.roc_auc_score(y_gt, y_pred)\n",
    "    precision, recall, _thresholds = sklearn.metrics.precision_recall_curve(y_gt, y_pred)\n",
    "    aupr = sklearn.metrics.auc(recall, precision)\n",
    "    print(\"\\n| Test Epoch #%d\\t AUC: %.2f AUPR: %2f\\n\" %(epoch,auc,aupr))  \n",
    "    test_log.write('Epoch:%d\\t AUC:%.2f AUPR: %2f\\n'%(epoch,auc,aupr))\n",
    "    test_log.flush()  \n",
    "    return auc, aupr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_train(model,all_loss):    \n",
    "    model.eval()\n",
    "    losses = torch.zeros(len(eval_loader.dataset))    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(eval_loader, desc=\"eval train\", file=sys.stdout) as iterator:\n",
    "            for batch_idx, (inputs, targets, index) in enumerate(iterator):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda() \n",
    "                outputs = model(inputs) \n",
    "                loss = CE(outputs, targets)  \n",
    "                for b in range(inputs.size(0)):\n",
    "                    losses[index[b]]=loss[b]       \n",
    "                    \n",
    "    losses = (losses-losses.min())/(losses.max()-losses.min())    \n",
    "    all_loss.append(losses)\n",
    "\n",
    "    # fit a two-component GMM to the loss\n",
    "    input_loss = losses.reshape(-1,1)\n",
    "    gmm = GaussianMixture(n_components=2,max_iter=10,tol=1e-2,reg_covar=5e-4)\n",
    "    gmm.fit(input_loss)\n",
    "    prob = gmm.predict_proba(input_loss) \n",
    "    prob = prob[:,gmm.means_.argmin()]         \n",
    "    return prob,all_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_rampup(current, warm_up, rampup_length=16):\n",
    "    current = np.clip((current-warm_up) / rampup_length, 0.0, 1.0)\n",
    "    return hyperParam[\"lambda_u\"]*float(current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemiLoss(object):\n",
    "    def __call__(self, outputs_x, targets_x, outputs_u, targets_u, epoch, warm_up):\n",
    "        probs_u = torch.softmax(outputs_u, dim=1)\n",
    "\n",
    "        Lx = -torch.mean(torch.sum(F.log_softmax(outputs_x, dim=1) * targets_x, dim=1))\n",
    "        Lu = torch.mean((probs_u - targets_u)**2)\n",
    "\n",
    "        return Lx, Lu, linear_rampup(epoch,warm_up)\n",
    "    \n",
    "class NegEntropy(object):\n",
    "    def __call__(self,y_pred):\n",
    "        log_likelihood = -1*nn.LogSoftmax(dim=1)(y_pred)\n",
    "        N, C = y_pred.size()\n",
    "        loss = torch.sum(torch.mul(log_likelihood, y_pred))/N\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    from utils.weight_init import weight_init\n",
    "    model = models.resnet18()\n",
    "    model.fc = nn.Linear(model.fc.in_features, hyperParam[\"num_class\"])\n",
    "    model.apply(weight_init)\n",
    "    model = model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_log = open('./%s'%(\"dividmix_test\")+'_stats.txt','w') \n",
    "test_log = open('./%s'%(\"dividmix_test\")+'_acc.txt','w')     \n",
    "bs = 32\n",
    "loader = WrapperLoader(batch_size=bs, num_workers=16, log=stats_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Building net\n"
     ]
    }
   ],
   "source": [
    "print('| Building net')\n",
    "net1 = create_model()\n",
    "net2 = create_model()\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SemiLoss()\n",
    "optimizer1 = torch.optim.SGD(net1.parameters(), lr=1e-4, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer2 = torch.optim.SGD(net2.parameters(), lr=1e-4, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "CE = nn.CrossEntropyLoss(reduction='none')\n",
    "CEloss = nn.CrossEntropyLoss()\n",
    "conf_penalty = NegEntropy()\n",
    "\n",
    "all_loss = [[],[]] # save the history of losses from two networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "lr = 1e-4\n",
    "p_threshold = 0.5\n",
    "warm_up = 5\n",
    "max_value = 0.8\n",
    "for epoch in range(0, num_epochs+1):   \n",
    "    print(\"Epoch [{}/{}]\".format(epoch, num_epochs))\n",
    "    if epoch >= 10:\n",
    "        lr /= 10      \n",
    "    for param_group in optimizer1.param_groups:\n",
    "        param_group['lr'] = lr       \n",
    "    for param_group in optimizer2.param_groups:\n",
    "        param_group['lr'] = lr              \n",
    "        \n",
    "    test_loader = loader.run('test')   \n",
    "    eval_loader = loader.run('eval_train')\n",
    "    \n",
    "    if epoch<warm_up:      \n",
    "        warmup_trainloader = loader.run('warmup')\n",
    "        print('Warmup Net1')\n",
    "        warmup(epoch,net1,optimizer1,warmup_trainloader)    \n",
    "        print('Warmup Net2')\n",
    "        warmup(epoch,net2,optimizer2,warmup_trainloader)   \n",
    "    else:         \n",
    "        prob1,all_loss[0]=eval_train(net1,all_loss[0])   \n",
    "        prob2,all_loss[1]=eval_train(net2,all_loss[1])          \n",
    "               \n",
    "        pred1 = (prob1 > hyperParam[\"p_threshold\"])      \n",
    "        pred2 = (prob2 > hyperParam[\"p_threshold\"])      \n",
    "        \n",
    "        print('Train Net1')\n",
    "        labeled_trainloader, unlabeled_trainloader = loader.run('train',pred2,prob2) # co-divide\n",
    "        train(epoch,net1,net2,optimizer1,labeled_trainloader, unlabeled_trainloader) # train net1  \n",
    "        \n",
    "        print('Train Net2')\n",
    "        labeled_trainloader, unlabeled_trainloader = loader.run('train',pred1,prob1) # co-divide\n",
    "        train(epoch,net2,net1,optimizer2,labeled_trainloader, unlabeled_trainloader) # train net2         \n",
    "    \n",
    "    print('Valid')\n",
    "    auc, aupr = test(epoch,net1,net2)  \n",
    "    if max_value < auc:\n",
    "        model_name1 = \"/data/weight/DivideMix_ResNet18_1\"      \n",
    "        model_name2 = \"/data/weight/DivideMix_ResNet18_2\"\n",
    "        torch.save(net1.state_dict(), model_name1)\n",
    "        torch.save(net2.state_dict(), model_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_loader = loader.run('eval_train')\n",
    "net1 = create_model()\n",
    "net1.cuda()\n",
    "net1.eval()\n",
    "net1.load_state_dict(torch.load(\"/data/weight/DivideMix_ResNet18_1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net1.eval()\n",
    "net2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/tcga/cohort_count.pkl\", \"rb\") as fp:\n",
    "    raw_TMB_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using_npy = \"/data/tcga/kmeans_cluster_32/\"\n",
    "# using_npy = \"/data/tcga/512densenpy/\"\n",
    "using_npy = \"/data/tcga/512denseTumor/\"\n",
    "valid_npy_pos = []\n",
    "valid_npy_neg = []\n",
    "valid_npy_normal = []\n",
    "for npy in sorted(glob.glob(os.path.join(using_npy, \"*.npy\"))):    \n",
    "    patient = npy.split(\"/\")[-1][:12]\n",
    "    if npy.split(\"/\")[-1][13] == \"1\":\n",
    "        valid_npy_normal.append(npy)\n",
    "    elif patient in valid_patient:\n",
    "        if lookup[patient] == 1:\n",
    "            valid_npy_pos.append(npy)\n",
    "        else:\n",
    "            valid_npy_neg.append(npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TT_positive_pred = {}\n",
    "with tqdm(valid_npy_pos, desc=\"test\", file=sys.stdout) as iterator:\n",
    "    for npy in iterator:\n",
    "        svs_name = npy.split(\"/\")[-1][:-4]\n",
    "        patient_name = svs_name[:12]\n",
    "        x_y_pairs = np.load(npy)\n",
    "        image_names = [\"{}_{}_{}.jpg\".format(svs_name, x, y) for x, y in x_y_pairs]\n",
    "        \n",
    "        test_dataset = CustomDataset(\n",
    "            image_names,\n",
    "            augmentation=get_validation_augmentation(),\n",
    "            preprocessing=get_preprocessing()\n",
    "        )\n",
    "        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=16, pin_memory=True)\n",
    "        patch_predictions = np.array([])\n",
    "        svs_pred = [0,0]\n",
    "        for images, labels, patch_names in test_loader:\n",
    "            with torch.no_grad():\n",
    "                outputs1 = net1.forward(images.cuda())\n",
    "                outputs2 = net2.forward(images.cuda())\n",
    "            outputs = outputs1 + outputs2\n",
    "            outputs = torch.softmax(outputs, dim=1)\n",
    "            outputs = outputs[:, 1].detach().cpu().numpy()\n",
    "            patch_predictions = np.concatenate((patch_predictions, outputs))\n",
    "            for p in outputs:\n",
    "                if p > 0.5:\n",
    "                    svs_pred[1] += 1\n",
    "                else:\n",
    "                    svs_pred[0] += 1\n",
    "        y_pred = svs_pred[1]/(svs_pred[0]+svs_pred[1])\n",
    "        if patient_name not in TT_positive_pred:\n",
    "            TT_positive_pred[patient_name] = []\n",
    "        TT_positive_pred[patient_name].append(y_pred)\n",
    "        \n",
    "        title = \"{}_gt={}_pred={:.4f}_raw={}\".format(svs_name, lookup[patient_name], y_pred, raw_TMB_dict[patient_name])\n",
    "        data_process.stitch.stitch(wsi_name = svs_name, x_y_pairs = x_y_pairs, preds = patch_predictions, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TT_positive_pred = {}\n",
    "with tqdm(valid_npy_pos, desc=\"test\", file=sys.stdout) as iterator:\n",
    "    for npy in iterator:\n",
    "        svs_name = npy.split(\"/\")[-1][:-4]\n",
    "        patient_name = svs_name[:12]\n",
    "        x_y_pairs = np.load(npy)\n",
    "        image_names = [\"{}_{}_{}.jpg\".format(svs_name, x, y) for x, y in x_y_pairs]\n",
    "        \n",
    "        test_dataset = CustomDataset(\n",
    "            image_names,\n",
    "            augmentation=get_validation_augmentation(),\n",
    "            preprocessing=get_preprocessing()\n",
    "        )\n",
    "        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=16, pin_memory=True)\n",
    "        patch_predictions = np.array([])\n",
    "        svs_pred = [0,0]\n",
    "        for images, labels, patch_names in test_loader:\n",
    "            with torch.no_grad():\n",
    "                outputs1 = net1.forward(images.cuda())\n",
    "                outputs2 = net2.forward(images.cuda())\n",
    "            outputs = outputs1 + outputs2\n",
    "            outputs = torch.softmax(outputs, dim=1)\n",
    "            outputs = outputs[:, 1].detach().cpu().numpy()\n",
    "            patch_predictions = np.concatenate((patch_predictions, outputs))\n",
    "            for p in outputs:\n",
    "                if p > 0.5:\n",
    "                    svs_pred[1] += 1\n",
    "                else:\n",
    "                    svs_pred[0] += 1\n",
    "        y_pred = svs_pred[1]/(svs_pred[0]+svs_pred[1])\n",
    "        if patient_name not in TT_positive_pred:\n",
    "            TT_positive_pred[patient_name] = []\n",
    "        TT_positive_pred[patient_name].append(y_pred)\n",
    "        \n",
    "        title = \"{}_gt={}_pred={:.4f}_raw={}\".format(svs_name, lookup[patient_name], y_pred, raw_TMB_dict[patient_name])\n",
    "        data_process.stitch.stitch(wsi_name = svs_name, x_y_pairs = x_y_pairs, preds = patch_predictions, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TT_negative_pred = {}\n",
    "with tqdm(valid_npy_neg, desc=\"test\", file=sys.stdout) as iterator:\n",
    "    for npy in iterator:\n",
    "        svs_name = npy.split(\"/\")[-1][:-4]\n",
    "        patient_name = svs_name[:12]\n",
    "        x_y_pairs = np.load(npy)\n",
    "        image_names = [\"{}_{}_{}.jpg\".format(svs_name, x, y) for x, y in x_y_pairs]\n",
    "        \n",
    "        test_dataset = CustomDataset(\n",
    "            image_names,\n",
    "            augmentation=get_validation_augmentation(),\n",
    "            preprocessing=get_preprocessing()\n",
    "        )\n",
    "        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=16, pin_memory=True)\n",
    "        patch_predictions = np.array([])\n",
    "        svs_pred = [0,0]\n",
    "        for images, labels, patch_names in test_loader:\n",
    "            with torch.no_grad():\n",
    "                outputs1 = net1.forward(images.cuda())\n",
    "                outputs2 = net2.forward(images.cuda())\n",
    "            outputs = outputs1 + outputs2\n",
    "            outputs = torch.softmax(outputs, dim=1)\n",
    "            outputs = outputs[:, 1].detach().cpu().numpy()\n",
    "            patch_predictions = np.concatenate((patch_predictions, outputs))\n",
    "            for p in outputs:\n",
    "                if p > 0.5:\n",
    "                    svs_pred[1] += 1\n",
    "                else:\n",
    "                    svs_pred[0] += 1\n",
    "        y_pred = svs_pred[1]/(svs_pred[0]+svs_pred[1])\n",
    "        if patient_name not in TT_negative_pred:\n",
    "            TT_negative_pred[patient_name] = []\n",
    "        TT_negative_pred[patient_name].append(y_pred)\n",
    "        \n",
    "        title = \"{}_gt={}_pred={:.4f}_raw={}\".format(svs_name, lookup[patient_name], y_pred, raw_TMB_dict[patient_name])\n",
    "        data_process.stitch.stitch(wsi_name = svs_name, x_y_pairs = x_y_pairs, preds = patch_predictions, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [np.amax(values) for key, values in TT_positive_pred.items()] \\\n",
    "        + [np.amax(values) for key, values in TT_negative_pred.items()]\n",
    "gt = [1]*len(TT_positive_pred) + [0]*len(TT_negative_pred)\n",
    "print(roc_auc_score(gt, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, patches, augmentation = None, preprocessing = None):\n",
    "        self.patches = patches\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        image_name = self.patches[i]\n",
    "        svs_name = image_name.split(\"_\")[0]\n",
    "        full_path = os.path.join(IMAGE_FOLDER, svs_name, image_name)\n",
    "        image = data_process.wsi_utils.vips_get_image(full_path)\n",
    "        \n",
    "        cls = lookup[image_name[:12]]\n",
    "        \n",
    "        if(self.augmentation):\n",
    "            sampled = self.augmentation(image = image)\n",
    "            _input = sampled['image']\n",
    "            \n",
    "        if(self.preprocessing):\n",
    "            sampled = self.preprocessing(image = _input)\n",
    "            _input = sampled['image']\n",
    "            \n",
    "        return _input, cls, image_name[:-4]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
